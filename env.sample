# vLLM Environment Variables for moonshotai/Kimi-K2-Instruct
# MoE model: 1T total params, 32B activated params
# Model weights are in block-fp8 format, KV cache in FP8
# Optimized for 8x B200 DGX (180GB VRAM each = 1.44TB total)

# Model Configuration
MODEL_NAME=moonshotai/Kimi-K2-Instruct
SERVED_MODEL_NAME=kimi-k2

# API Configuration
PORT=8080
HOST=0.0.0.0

# Model Parameters
MAX_MODEL_LEN=131072   # Start with 64k for stability, can increase to 128k
MAX_NUM_SEQS=256      # Kimi recommended
MAX_NUM_BATCHED_TOKENS=32768  # Increased for better throughput on B200s

# Hardware Configuration (8x B200 GPUs with 180GB VRAM each)
TENSOR_PARALLEL_SIZE=8        # Use TP=8 for single node deployment
GPU_MEMORY_UTILIZATION=0.95   # Can use higher utilization on B200s

# Performance Settings
# Model weights are already in block-fp8 format
# DTYPE=auto                    # Will auto-detect FP8 from model
QUANTIZATION=fp8           # Enable FP8 KV cache for memory efficiency
ENFORCE_EAGER=false
ENABLE_CHUNKED_PREFILL=true
DISABLE_SLIDING_WINDOW=false

# Tool calling support (Kimi specific)
ENABLE_AUTO_TOOL_CHOICE=true
TOOL_CALL_PARSER=kimi_k2

# Download Settings
TRUST_REMOTE_CODE=true
DOWNLOAD_DIR=/models

# Optional: HuggingFace Token (if model is gated)
# HUGGING_FACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Optional: API Key for authentication
# API_KEY=your_api_key_here

# Logging
DISABLE_LOG_STATS=false

# CUDA optimizations
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
#VLLM_USE_V1=0
#VLLM_ATTENTION_BACKEND=FLASH_ATTN